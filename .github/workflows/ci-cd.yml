name: CI-CD

on:
  push:
    branches: ["main"]
  workflow_dispatch:
    inputs:
      environment:
        description: "Deployment environment (dev|staging|prod)"
        type: choice
        options:
          - dev
          - staging
          - prod
        default: "dev"
        required: true

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: ${{ secrets.AWS_REGION }}
  ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}
  EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
  ROLE_TO_ASSUME: ${{ secrets.ROLE_TO_ASSUME }}
  S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
  ECR_BACKEND_REPO: ${{ secrets.ECR_BACKEND_REPO }}
  ECR_FRONTEND_REPO: ${{ secrets.ECR_FRONTEND_REPO }}

jobs:
  test-and-build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 18

      - name: Backend - Install and Test
        working-directory: backend
        run: |
          npm ci
          npm test

      - name: Frontend - Install and Build
        working-directory: frontend
        run: |
          npm ci
          npm run build

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ env.ROLE_TO_ASSUME }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and Push Backend Image
        run: |
          BACKEND_IMAGE=${{ env.ECR_REGISTRY }}/${{ env.ECR_BACKEND_REPO }}:${{ github.sha }}
          docker build -t $BACKEND_IMAGE ./backend
          docker push $BACKEND_IMAGE

      - name: Build and Push Frontend Image
        run: |
          FRONTEND_IMAGE=${{ env.ECR_REGISTRY }}/${{ env.ECR_FRONTEND_REPO }}:${{ github.sha }}
          docker build -t $FRONTEND_IMAGE ./frontend
          docker push $FRONTEND_IMAGE

      - name: Upload frontend assets to S3 (optional)
        if: env.S3_BUCKET_NAME != ''
        run: |
          aws s3 sync frontend/dist s3://${{ env.S3_BUCKET_NAME }}

  deploy:
    needs: test-and-build
    runs-on: ubuntu-latest
    env:
      ENVIRONMENT: ${{ github.event.inputs.environment || 'dev' }}
      NAMESPACE: ${{ github.event.inputs.environment || 'dev' }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ env.ROLE_TO_ASSUME }}

      - name: Install kubectl, kustomize, and helm
        run: |
          curl -L -o kubectl https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
          sudo mv kustomize /usr/local/bin/
          curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Debug AWS identity and EKS cluster
        run: |
          aws sts get-caller-identity
          aws eks describe-cluster --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}

      - name: Diagnose aws-auth mapping
        run: |
          echo "Attempting to read aws-auth ConfigMap (requires role mapping)"
          kubectl -n kube-system get configmap aws-auth -o yaml || echo "Cannot read aws-auth; likely IAM role not mapped in aws-auth ConfigMap"

      - name: Verify cluster access
        run: |
          kubectl cluster-info || { echo "kubectl cannot access cluster"; exit 1; }
          kubectl get nodes || { echo "kubectl cannot access cluster"; exit 1; }

      - name: Create ingress-nginx values file
        if: env.ENVIRONMENT != 'dev'
        run: |
          Set-Content -Path C:\temp\ingress-values.yaml -Value @"
controller:
  admissionWebhooks:
    enabled: false
    patch:
      enabled: false
  deployment:
    strategy:
      type: Recreate
"@

      - name: Clean stuck ingress-nginx admission jobs
        if: env.ENVIRONMENT != 'dev'
        run: |
          kubectl -n ingress-nginx delete job ingress-nginx-admission-create --ignore-not-found=true
          kubectl -n ingress-nginx delete job ingress-nginx-admission-patch --ignore-not-found=true

      - name: Install/Upgrade ingress-nginx
        if: env.ENVIRONMENT != 'dev'
        run: |
          kubectl create namespace ingress-nginx --dry-run=client -o yaml | kubectl apply -f -
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx \
            --values C:\temp\ingress-values.yaml \
            --wait --timeout 10m --atomic

      - name: Set images for overlay
        working-directory: k8s/overlays/${{ env.ENVIRONMENT }}
        run: |
          kustomize edit set image REPLACE_BACKEND_IMAGE=${{ env.ECR_REGISTRY }}/${{ env.ECR_BACKEND_REPO }}:${{ github.sha }}
          kustomize edit set image REPLACE_FRONTEND_IMAGE=${{ env.ECR_REGISTRY }}/${{ env.ECR_FRONTEND_REPO }}:${{ github.sha }}

      - name: Create TLS secret for environment
        run: |
          set -e
          NAMESPACE=${{ env.NAMESPACE }}
          ENV=${{ env.ENVIRONMENT }}
          if [ "$ENV" = "dev" ]; then
            CERT="${{ secrets.TLS_CERT_DEV }}"
            KEY="${{ secrets.TLS_KEY_DEV }}"
            SECRET_NAME="tls-dev"
          elif [ "$ENV" = "prod" ]; then
            CERT="${{ secrets.TLS_CERT_PROD }}"
            KEY="${{ secrets.TLS_KEY_PROD }}"
            SECRET_NAME="tls-prod"
          else
            CERT="${{ secrets.TLS_CERT_STAGING }}"
            KEY="${{ secrets.TLS_KEY_STAGING }}"
            SECRET_NAME="tls-placeholder"
          fi
          mkdir -p /tmp/tls
          if [ -n "$CERT" ] && [ -n "$KEY" ]; then
            printf "%s" "$CERT" > /tmp/tls/tls.crt
            printf "%s" "$KEY" > /tmp/tls/tls.key
          else
            openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls/tls.key -out /tmp/tls/tls.crt -subj "/CN=staging.local"
          fi
          kubectl -n "$NAMESPACE" create secret tls "$SECRET_NAME" --cert=/tmp/tls/tls.crt --key=/tmp/tls/tls.key --dry-run=client -o yaml | kubectl apply -f -
          rm -rf /tmp/tls

      - name: Apply manifests
        run: |
          kubectl apply -k x:\DevOps\Documents\DevOps-task\k8s\overlays\${{ env.ENVIRONMENT }}

      - name: Print frontend ELB URL (dev only)
        if: env.ENVIRONMENT == 'dev'
        run: |
          $hostname = kubectl -n dev get svc frontend-svc -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'
          echo "Frontend ELB: http://$hostname"
          if (-not $hostname) { echo "Waiting for ELB hostname..." }
          $retries = 30
          while (-not $hostname -and $retries -gt 0) {
            Start-Sleep -Seconds 10
            $hostname = kubectl -n dev get svc frontend-svc -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'
            $retries -= 1
          }
          echo "Frontend ELB: http://$hostname"

      - name: Wait for rollout
        run: |
          kubectl -n ${{ env.NAMESPACE }} rollout status deploy/backend --timeout=180s || echo "backend rollout failed"
          kubectl -n ${{ env.NAMESPACE }} rollout status deploy/frontend --timeout=180s || echo "frontend rollout failed"

      - name: Rollback on failure
        if: failure()
        run: |
          kubectl -n ${{ env.NAMESPACE }} rollout undo deploy/backend || true
          kubectl -n ${{ env.NAMESPACE }} rollout undo deploy/frontend || true